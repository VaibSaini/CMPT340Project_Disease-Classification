# -*- coding: utf-8 -*-
"""Random Forest.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EBlivndquzorg40tMxh80pHJBTgjux4z

# Importing libraries and pre-processed data
"""

# Commented out IPython magic to ensure Python compatibility.
!pip install split-folders

import feather
from numpy import argmax, where
import wave
# %matplotlib inline
import IPython.display
import random
from PIL import Image
import pathlib
import csv
import glob
from google.colab import drive
import librosa
import librosa.display
import matplotlib.pyplot as plt
import numpy as np
import os
from os import listdir
from os.path import isfile, join
import pandas as pd 
import pickle as pkl
import seaborn as sns
import scipy
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# ML Libraries
from sklearn.datasets import make_blobs
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import mean_squared_error,r2_score, completeness_score, accuracy_score, roc_auc_score, classification_report, confusion_matrix
from sklearn.metrics.cluster import contingency_matrix
from sklearn.model_selection import train_test_split,cross_val_score
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import Conv1D, Conv2D, MaxPooling2D, MaxPooling1D, Dense, Flatten, Dropout, SeparableConv1D
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import plot_model,to_categorical
import keras
from keras import layers
from keras.layers import Activation, Dense, Dropout, Conv2D, Flatten, MaxPooling2D, GlobalMaxPooling2D, GlobalAveragePooling1D, AveragePooling2D, Input, Add
from keras.models import Sequential
from tensorflow.keras.optimizers import SGD
from keras.preprocessing.image import ImageDataGenerator
import splitfolders

# Commented out IPython magic to ensure Python compatibility.
drive.mount('/content/gdrive', force_remount=True)
# %cd gdrive/MyDrive/'CMPT 340 Project'/audio_and_txt_files

dataset = pd.read_pickle('Final_Data.pkl') 

dataset_non_Augmented = dataset[dataset['rand int i'] == -1]
dataset_Augmented = dataset[dataset['rand int i'] != -1]
dataset_Augmented.reset_index(inplace=True)

"""Now we have the joined dataset as well as the split versions to properly organize our models testing and training

## REMOVE
"""

dataset.info(),dataset_non_Augmented.info(),dataset_Augmented.info()

dataset = dataset[ dataset["Diagnosis"] != "Asthma"]
dataset = dataset[ dataset["Diagnosis"] != "LRTI"  ]
dataset.reset_index(inplace=True)

dataset_non_Augmented = dataset_non_Augmented[dataset_non_Augmented["Diagnosis"] != "Asthma"]
dataset_non_Augmented = dataset_non_Augmented[dataset_non_Augmented["Diagnosis"] != "LRTI"]
dataset_non_Augmented.reset_index(inplace=True)

dataset_Augmented = dataset_Augmented[dataset_Augmented["Diagnosis"] != "Asthma"]
dataset_Augmented = dataset_Augmented[dataset_Augmented["Diagnosis"] != "LRTI"]
dataset_Augmented.reset_index(inplace=True)

dataset.info(),dataset_non_Augmented.info(),dataset_Augmented.info()

"""## REMOVE"""

dataset_sequential_whole = dataset.copy() 
dataset_sequential_Augmented = dataset_Augmented.copy()
dataset_sequential_non_Augmented = dataset_non_Augmented.copy()
dataset

dataset["Diagnosis"] = dataset["Diagnosis"].astype('category')
dataset["Diagnosis"] = dataset["Diagnosis"].cat.codes
dataset['Binary_diagnosis'] = (dataset['Binary_diagnosis'] == "Healthy").astype(int)

dataset_non_Augmented["Diagnosis"] = dataset_non_Augmented["Diagnosis"].astype('category')
dataset_non_Augmented["Diagnosis"] = dataset_non_Augmented["Diagnosis"].cat.codes
dataset_non_Augmented['Binary_diagnosis'] = (dataset_non_Augmented['Binary_diagnosis'] == "Healthy").astype(int)

dataset_Augmented["Diagnosis"] = dataset_Augmented["Diagnosis"].astype('category')
dataset_Augmented["Diagnosis"] = dataset_Augmented["Diagnosis"].cat.codes
dataset_Augmented['Binary_diagnosis'] = (dataset_Augmented['Binary_diagnosis'] == "Healthy").astype(int)

"""Cat codes work by doing a label encoding alphabetically, so URTI will be 5 because it is lowest """

print("Number of Healthy Patients: ",(dataset['Binary_diagnosis'] == 1).sum())
print("Number of Unhealthy Patients: ",(dataset['Binary_diagnosis'] == 0).sum())
print("\n")
#print("Number of Patients with Asthma are: ",(dataset['Diagnosis'] == 0).sum())
print("Number of Patients with Bronchiectasis are: ",(dataset['Diagnosis'] == 0).sum())
print("Number of Patients with Bronchiolitis are: ",(dataset['Diagnosis'] == 1).sum())
print("Number of Patients with COPD are: ",(dataset['Diagnosis'] == 2).sum())
print("Number of Patients that are Healthy, are: ",(dataset['Diagnosis'] == 3).sum())
#print("Number of Patients with LRTI are: ",(dataset['Diagnosis'] == 5).sum())
print("Number of Patients with Pneumonia are: ",(dataset['Diagnosis'] == 4).sum())
print("Number of Patients with URTI are: ",(dataset['Diagnosis'] == 5).sum())

"""## Multi Classification of Whole Dataset"""

features = dataset.drop(columns = ['Diagnosis','Binary_diagnosis','Patient number','Recording index',"rand int i"])

targets = dataset[['Diagnosis']]

X_train, X_test, y_train, y_test=train_test_split(features, targets, test_size=0.2) # rand state sets a seed so that it will be the same

X_train_whole_dataset = X_train.values
X_test_whole_dataset = X_test.values

y_train_multi_whole_dataset = y_train.values.reshape(-1,)
y_test_multi_whole_dataset = y_test.values.reshape(-1,)

"""## Binary Classification of Whole Dataset"""

# targets = dataset[['Binary_diagnosis']]

# y = targets.values.reshape(-1,)

y_train_binary_whole_dataset = (y_train["Diagnosis"] == 4).astype(int).values.reshape(-1,)
y_test_binary_whole_dataset = (y_test["Diagnosis"] == 4).astype(int).values.reshape(-1,)

"""## Multi Classification of Augmented Dataset"""

features = dataset_Augmented.drop(columns = ['Diagnosis','Binary_diagnosis','Patient number','Recording index',"rand int i"])
targets = dataset_Augmented[['Diagnosis']]

X_train, X_test, y_train, y_test=train_test_split(features, targets, test_size=0.2 , random_state = 5000) # rand state sets a seed so that it will be the same

X_train_augmented_dataset = X_train.values
X_test_augmented_dataset = X_test.values

y_train_multi = y_train.values.reshape(-1,)
y_test_multi = y_test.values.reshape(-1,)

"""## Binary Classification of Augmented Dataset

"""

y_train_binary = (y_train["Diagnosis"] == 4).astype(int).values.reshape(-1,)
y_test_binary = (y_test["Diagnosis"] == 4).astype(int).values.reshape(-1,)

"""Training on the augmented dataset will be interesting, especially in relation to non-augmented

##Non-Augmented Dataset

We want to use the same data rows, so all we do is change the X_trains to be from the non augmented dataset, now the binary and multi classification values will stay the same
"""

X_train_non = dataset_non_Augmented.loc[X_train.index].drop(columns = ['Diagnosis','Binary_diagnosis','Patient number','Recording index',"rand int i"])
X_test_non = dataset_non_Augmented.loc[X_test.index].drop(columns = ['Diagnosis','Binary_diagnosis','Patient number','Recording index',"rand int i"])

X_train_non_augmented_dataset =  X_train_non.values
X_test_non_augmented_dataset = X_test_non.values

"""Training on the augmented dataset will be interesting, especially in relation to non-augmented

#Random Forest Model
"""

# Creates the model which will later be fitted by training & test sets
random_forest_model = RandomForestClassifier(n_estimators=300)

"""##Multi Classification of Whole Dataset using RF

"""

random_forest_model.fit(X_train_whole_dataset, y_train_multi_whole_dataset)

# Print training & validation score
print("Random Forest training score: ", random_forest_model.score(X_train_whole_dataset, y_train_multi_whole_dataset))
print("Random Forest validation score: ", random_forest_model.score(X_test_whole_dataset, y_test_multi_whole_dataset))

# Commented out IPython magic to ensure Python compatibility.
# %cd ../report_figures/

y_train_predict = random_forest_model.predict(X_train_whole_dataset)
y_test_predict = random_forest_model.predict(X_test_whole_dataset)

"""##Binary Classification of Whole Dataset using RF"""

random_forest_model.fit(X_train_whole_dataset, y_train_binary_whole_dataset)

# Print training & validation score
print("Random Forest training score: ", random_forest_model.score(X_train_whole_dataset, y_train_binary_whole_dataset))
print("Random Forest validation score: ", random_forest_model.score(X_test_whole_dataset, y_test_binary_whole_dataset))

"""## Multi Classification of Augmented Dataset using RF"""

random_forest_model.fit(X_train_augmented_dataset, y_train_multi)

# Print training & validation score
print("Random Forest training score: ", random_forest_model.score(X_train_augmented_dataset, y_train_multi))
print("Random Forest validation score: ", random_forest_model.score(X_test_augmented_dataset, y_test_multi))

"""## Binary Classification of Augmented Dataset using RF

"""

random_forest_model.fit(X_train_augmented_dataset, y_train_binary)

# Print training & validation score
print("Random Forest training score: ", random_forest_model.score(X_train_augmented_dataset, y_train_binary))
print("Random Forest validation score: ", random_forest_model.score(X_test_augmented_dataset, y_test_binary))

"""## Multi Classification of Non-Augmented Dataset using RF"""

random_forest_model.fit(X_train_non_augmented_dataset, y_train_multi)

# Print training & validation score
print("Random Forest training score: ", random_forest_model.score(X_train_non_augmented_dataset, y_train_multi))
print("Random Forest validation score: ", random_forest_model.score(X_test_non_augmented_dataset, y_test_multi))

"""## Binary Classification of Non-Augmented Dataset using RF



"""

random_forest_model.fit(X_train_non_augmented_dataset, y_train_binary)

# Print training & validation score
print("Random Forest training score: ", random_forest_model.score(X_train_non_augmented_dataset, y_train_binary))
print("Random Forest validation score: ", random_forest_model.score(X_test_non_augmented_dataset, y_test_binary))