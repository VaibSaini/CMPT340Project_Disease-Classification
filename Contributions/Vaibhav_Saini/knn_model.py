# -*- coding: utf-8 -*-
"""Copy of Models + KNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OAi1yrE9QL9IY8S5x2sEGToblz_nIEkI

# Importing libraries and pre-processed data
"""

import numpy as np
import pandas as pd 
import os
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, Conv2D, MaxPooling2D, MaxPooling1D, Dense, Flatten, Dropout, SeparableConv1D
import matplotlib.pyplot as plt
import pickle as pkl
import seaborn as sns
import librosa
import librosa.display
from sklearn.model_selection import train_test_split,cross_val_score
from sklearn.metrics import mean_squared_error,r2_score, completeness_score, accuracy_score
from sklearn.metrics.cluster import contingency_matrix
from sklearn.preprocessing import StandardScaler
from os import listdir
from os.path import isfile, join
from tensorflow.keras.utils import plot_model,to_categorical
#from google.colab import drive
import scipy
import glob
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

#Import libaries for KNN
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

# Commented out IPython magic to ensure Python compatibility.
#drive.mount('/content/gdrive', force_remount=True)
# %cd gdrive/MyDrive/'CMPT 340 Project'/audio_and_txt_files

dataset = pd.read_pickle('Final_Data.pkl') 

dataset_non_Augmented = dataset[dataset['rand int i'] == -1]
dataset_Augmented = dataset[dataset['rand int i'] != -1]
dataset_Augmented.reset_index(inplace=True)

"""Now we have the joined dataset as well as the split versions to properly organize our models testing and training"""

dataset_sequential = dataset.copy() 

#plt.figure(figsize = (33,33))
#sns.heatmap(dataset.corr().round(1), annot = True)

dataset["Diagnosis"] = dataset["Diagnosis"].astype('category')
dataset["Diagnosis"] = dataset["Diagnosis"].cat.codes
dataset['Binary_diagnosis'] = (dataset['Binary_diagnosis'] == "Healthy").astype(int)

dataset_non_Augmented["Diagnosis"] = dataset_non_Augmented["Diagnosis"].astype('category')
dataset_non_Augmented["Diagnosis"] = dataset_non_Augmented["Diagnosis"].cat.codes
dataset_non_Augmented['Binary_diagnosis'] = (dataset_non_Augmented['Binary_diagnosis'] == "Healthy").astype(int)

dataset_Augmented["Diagnosis"] = dataset_Augmented["Diagnosis"].astype('category')
dataset_Augmented["Diagnosis"] = dataset_Augmented["Diagnosis"].cat.codes
dataset_Augmented['Binary_diagnosis'] = (dataset_Augmented['Binary_diagnosis'] == "Healthy").astype(int)

correlation_heatmap = dataset[['Patient number', 'Sex' ,	'Age'	,'Diagnosis',	'Binary_diagnosis',	'zero_crossing',	'centroids',	'energy',	'new BMI']]
#plt.figure(figsize = (33,33))
#sns.heatmap(correlation_heatmap.corr().round(1), annot = True)

print("Number of Healthy Patients: ",(dataset['Binary_diagnosis'] == 1).sum())
print("Number of Unhealthy Patients: ",(dataset['Binary_diagnosis'] == 0).sum())

print("Number of Patients with Asthma are: ",(dataset['Diagnosis'] == 0).sum())
print("Number of Patients with Bronchiectasis are: ",(dataset['Diagnosis'] == 1).sum())
print("Number of Patients with Bronchiolitis are: ",(dataset['Diagnosis'] == 2).sum())
print("Number of Patients with COPD are: ",(dataset['Diagnosis'] == 3).sum())
print("Number of Patients that are Healthy, are: ",(dataset['Diagnosis'] == 4).sum())
print("Number of Patients with LRTI are: ",(dataset['Diagnosis'] == 5).sum())
print("Number of Patients with Pneumonia are: ",(dataset['Diagnosis'] == 6).sum())
print("Number of Patients with URTI are: ",(dataset['Diagnosis'] == 7).sum())


"""## Multi Classification of Whole Dataset"""

features = dataset.drop(columns = ['Diagnosis','Binary_diagnosis','Patient number','Recording index',"rand int i"])

targets = dataset[['Diagnosis']]

X_train, X_test, y_train, y_test=train_test_split(features, targets, test_size=0.2) # rand state sets a seed so that it will be the same

X_train_whole_dataset = X_train.values
X_test_whole_dataset = X_test.values

y_train_multi_whole_dataset = y_train.values.reshape(-1,)
y_test_multi_whole_dataset = y_test.values.reshape(-1,)

"""## Binary Classification of Whole Dataset"""

# targets = dataset[['Binary_diagnosis']]

# y = targets.values.reshape(-1,)

y_train_binary_whole_dataset = (y_train["Diagnosis"] == 4).astype(int).values.reshape(-1,)
y_test_binary_whole_dataset = (y_test["Diagnosis"] == 4).astype(int).values.reshape(-1,)

"""## Multi Classification of Augmented Dataset"""

features = dataset_Augmented.drop(columns = ['Diagnosis','Binary_diagnosis','Patient number','Recording index',"rand int i"])
targets = dataset_Augmented[['Diagnosis']]

X_train, X_test, y_train, y_test=train_test_split(features, targets, test_size=0.2) # rand state sets a seed so that it will be the same

X_train_augmented_dataset = X_train.values
X_test_augmented_dataset = X_test.values

y_train_multi = y_train.values.reshape(-1,)
y_test_multi = y_test.values.reshape(-1,)

"""## Binary Classification of Augmented Dataset"""

y_train_binary = (y_train["Diagnosis"] == 4).astype(int).values.reshape(-1,)
y_test_binary = (y_test["Diagnosis"] == 4).astype(int).values.reshape(-1,)

"""Training on the augmented dataset will be interesting, especially in relation to non-augmented

We want to use the same data rows, so all we do is change the X_trains to be from the non augmented dataset, now the binary and multi classification values will stay the same
"""

X_train_non = dataset_non_Augmented.loc[X_train.index].drop(columns = ['Diagnosis','Binary_diagnosis','Patient number','Recording index',"rand int i"])
X_test_non = dataset_non_Augmented.loc[X_test.index].drop(columns = ['Diagnosis','Binary_diagnosis','Patient number','Recording index',"rand int i"])

X_train_non_augmented_dataset =  X_train_non.values
X_test_non_augmented_dataset = X_test_non.values

"""# KNN Model """

#setting up KNN model with the help of Grid search CV for hyperparameters tuning

k_range = list(range(3,21)) #compare classification results between 3-20 neighbours
param_grid = {
    'n_neighbors': k_range,
    'weights': ['uniform', 'distance'],
    'metric': ['euclidean', 'manhattan']
}

# Scaler to tranform the data before fitting to the model
scaler = StandardScaler()

"""## Whole dataset"""

#Fitting whole dataset
scaler.fit(X_train_whole_dataset)
x_train_scaled_whole = scaler.transform(X_train_whole_dataset)
x_test_scaled_whole = scaler.transform(X_test_whole_dataset)

#using PCA to reduce dimensionality
pca = PCA().fit(x_train_scaled_whole)
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('Number of Components')
plt.ylabel('Variance (%)')
plt.title('Variance vs No of Components for Whole Dataset')
plt.savefig('vc-whole.png')
plt.show()

"""### Multi classification"""

model = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5, n_jobs=-1, scoring='accuracy')
model.fit(x_train_scaled_whole, y_train_multi_whole_dataset)
print(" ")
print("---- Multi-disease classification on the whole dataset ----")
print(f'Model Training Score: {model.score(x_train_scaled_whole, y_train_multi_whole_dataset)}')
print(f'Model Testing Score: {model.score(x_test_scaled_whole, y_test_multi_whole_dataset)}')
print(" ")

y_predict_whole_multi = model.predict(x_test_scaled_whole)
print(f'Confusion Matrix: \n{confusion_matrix(y_test_multi_whole_dataset,y_predict_whole_multi)}')
print(" ")

# create confusion matrix heatmap for predictions
contingency = contingency_matrix(y_test_multi_whole_dataset,y_predict_whole_multi)
plt.subplots(figsize=(10,10))
sns.heatmap(contingency, annot=True)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Multi-disease classification on whole dataset (ACC = ' + str(round(model.score(x_test_scaled_whole, y_test_multi_whole_dataset),5)) + ')')
plt.savefig('heatmap-whole-multi.png')
plt.show()

"""### Binary classification"""


model = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5, n_jobs=-1, scoring='accuracy')
model.fit(x_train_scaled_whole, y_train_binary_whole_dataset)
print(" ")
print("---- Binary-disease classification on the whole dataset ----")
print(f'Model Training Score: {model.score(x_train_scaled_whole, y_train_binary_whole_dataset)}')
print(f'Model Testing Score: {model.score(x_test_scaled_whole, y_test_binary_whole_dataset)}')
print(" ")
y_predict_whole_bin = model.predict(x_test_scaled_whole)
print(f'Confusion Matrix: \n{confusion_matrix(y_test_binary_whole_dataset,y_predict_whole_bin)}')
print(" ")

# create confusion matrix heatmap for predictions
contingency = contingency_matrix(y_test_binary_whole_dataset,y_predict_whole_bin)
fig, ax = plt.subplots(figsize=(10,10))
sns.heatmap(contingency, annot=True)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Binary classification on whole dataset (ACC = ' + str(round(model.score(x_test_scaled_whole, y_test_binary_whole_dataset),5)) + ')')
plt.savefig('heatmap-whole-bin.png')
plt.show()

"""## Augmented dataset"""

#Fitting Augmented data
scaler.fit(X_train_augmented_dataset)
x_train_scaled_aug = scaler.transform(X_train_augmented_dataset)
x_test_scaled_aug = scaler.transform(X_test_augmented_dataset)

pca = PCA().fit(x_train_scaled_aug)
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('Number of Components')
plt.ylabel('Variance (%)')
plt.title('Variance vs No of Components for Augmented Dataset')
plt.savefig('vc-aug.png')
plt.show()

"""### Multi classification"""

model = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5, n_jobs=-1, scoring='accuracy')
model.fit(x_train_scaled_aug, y_train_multi)

print(" ")
print("---- Multi-disease classification on the augmented dataset ----")
print(f'Model Training Score: {model.score(x_train_scaled_aug, y_train_multi)}')
print(f'Model Testing Score: {model.score(x_test_scaled_aug, y_test_multi)}')
print(" ")
y_predict_aug_multi = model.predict(x_test_scaled_aug)
print(f'Confusion Matrix: \n{confusion_matrix(y_test_multi,y_predict_aug_multi)}')
print(" ")

# create confusion matrix heatmap for predictions
contingency = contingency_matrix(y_test_multi,y_predict_aug_multi)
fig, ax = plt.subplots(figsize=(10,10))
sns.heatmap(contingency, annot=True)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Multi-classification on augmented dataset (ACC = ' + str(round(model.score(x_test_scaled_aug, y_test_multi),5)) + ')')
plt.savefig('heatmap-aug-multi.png')
plt.show()

"""### Binary classification"""

model = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5, n_jobs=-1, scoring='accuracy')
model.fit(x_train_scaled_aug, y_train_binary)

print(" ")
print("---- Binary-disease classification on the augmented dataset ----")
print(f'Model Training Score: {model.score(x_train_scaled_aug, y_train_binary)}')
print(f'Model Testing Score: {model.score(x_test_scaled_aug, y_test_binary)}')
print(" ")
y_predict_aug_bin = model.predict(x_test_scaled_aug)
print(f'Confusion Matrix: \n{confusion_matrix(y_test_binary,y_predict_aug_bin)}')
print(" ")

# create confusion matrix heatmap for predictions
contingency = contingency_matrix(y_test_binary,y_predict_aug_bin)
fig, ax = plt.subplots(figsize=(10,10))
sns.heatmap(contingency, annot=True)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Binary classification on augmented dataset (ACC = ' + str(round(model.score(x_test_scaled_aug, y_test_binary),5)) + ')')
plt.savefig('heatmap-aug-bin.png')
plt.show()

"""## Non-Augmented dataset"""

scaler.fit(X_train_non_augmented_dataset)
x_train_scaled_na = scaler.transform(X_train_non_augmented_dataset)
x_test_scaled_na = scaler.transform(X_test_non_augmented_dataset)

pca = PCA().fit(x_train_scaled_na)
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('Number of Components')
plt.ylabel('Variance (%)')
plt.title('Variance vs No of Components for Non-Augmented Dataset')
plt.savefig('vc-na.png')
plt.show()

"""### Multi classification"""

model = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5, n_jobs=-1, scoring='accuracy')
model.fit(x_train_scaled_na, y_train_multi)

print(" ")
print("---- Multi-disease classification on the non-augmented dataset ----")
print(f'Model Training Score: {model.score(x_train_scaled_na, y_train_multi)}')
print(f'Model Testing Score: {model.score(x_test_scaled_na, y_test_multi)}')
print(" ")
y_predict_na_multi = model.predict(x_test_scaled_na)
print(f'Confusion Matrix: \n{confusion_matrix(y_test_multi,y_predict_na_multi)}')
print(" ")

# create confusion matrix heatmap for predictions
contingency = contingency_matrix(y_test_multi,y_predict_na_multi)
fig, ax = plt.subplots(figsize=(10,10))
sns.heatmap(contingency, annot=True)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Multi-classification on non-augmented dataset (ACC = ' + str(round(model.score(x_test_scaled_na, y_test_multi),5)) + ')')
plt.savefig('heatmap-na-multi.png')
plt.show()
"""### Binary classification """

model = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5, n_jobs=-1, scoring='accuracy')
model.fit(x_train_scaled_na, y_train_binary)

print(" ")
print("---- Multi-disease classification on the non-augmented dataset ----")
print(f'Model Training Score: {model.score(x_train_scaled_na, y_train_binary)}')
print(f'Model Testing Score: {model.score(x_test_scaled_na, y_test_binary)}')
print(" ")
y_predict_na_bin = model.predict(x_test_scaled_na)
print(f'Confusion Matrix: \n{confusion_matrix(y_test_binary,y_predict_na_bin)}')
print(" ")

# create confusion matrix heatmap for predictions
contingency = contingency_matrix(y_test_binary,y_predict_na_bin)
fig, ax = plt.subplots(figsize=(10,10))
sns.heatmap(contingency, annot=True)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Binary classification on non-augmented dataset (ACC = ' + str(round(model.score(x_test_scaled_na, y_test_binary),5)) + ')')
plt.savefig('heatmap-na-bin.png')
plt.show()