# -*- coding: utf-8 -*-
"""Birch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1X6fCFsDfqnp9IiepCpHAEhvtGr2QHZ-k

# Importing libraries and pre-processed data
"""

# Commented out IPython magic to ensure Python compatibility.
!pip install split-folders

import feather
from numpy import argmax, where
import wave
# %matplotlib inline
import IPython.display
import random
from PIL import Image
import pathlib
import csv
import glob
from google.colab import drive
import librosa
import librosa.display
import matplotlib.pyplot as plt
import numpy as np
import os
from os import listdir
from os.path import isfile, join
import pandas as pd 
import pickle as pkl
import seaborn as sns
import scipy
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# ML Libraries
from sklearn.datasets import make_blobs
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import mean_squared_error,r2_score, completeness_score, accuracy_score, roc_auc_score, classification_report, confusion_matrix
from sklearn.metrics.cluster import contingency_matrix
from sklearn.model_selection import train_test_split,cross_val_score
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import Conv1D, Conv2D, MaxPooling2D, MaxPooling1D, Dense, Flatten, Dropout, SeparableConv1D
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import plot_model,to_categorical
import keras
from keras import layers
from keras.layers import Activation, Dense, Dropout, Conv2D, Flatten, MaxPooling2D, GlobalMaxPooling2D, GlobalAveragePooling1D, AveragePooling2D, Input, Add
from keras.models import Sequential
from tensorflow.keras.optimizers import SGD
from keras.preprocessing.image import ImageDataGenerator
import splitfolders

# Commented out IPython magic to ensure Python compatibility.
drive.mount('/content/gdrive', force_remount=True)
# %cd gdrive/MyDrive/'CMPT 340 Project'/audio_and_txt_files

dataset = pd.read_pickle('Final_Data.pkl') 

dataset_non_Augmented = dataset[dataset['rand int i'] == -1]
dataset_Augmented = dataset[dataset['rand int i'] != -1]
dataset_Augmented.reset_index(inplace=True)

"""Now we have the joined dataset as well as the split versions to properly organize our models testing and training

## REMOVE
"""

dataset.info(),dataset_non_Augmented.info(),dataset_Augmented.info()

dataset = dataset[ dataset["Diagnosis"] != "Asthma"]
dataset = dataset[ dataset["Diagnosis"] != "LRTI"  ]
dataset.reset_index(inplace=True)

dataset_non_Augmented = dataset_non_Augmented[dataset_non_Augmented["Diagnosis"] != "Asthma"]
dataset_non_Augmented = dataset_non_Augmented[dataset_non_Augmented["Diagnosis"] != "LRTI"]
dataset_non_Augmented.reset_index(inplace=True)

dataset_Augmented = dataset_Augmented[dataset_Augmented["Diagnosis"] != "Asthma"]
dataset_Augmented = dataset_Augmented[dataset_Augmented["Diagnosis"] != "LRTI"]
dataset_Augmented.reset_index(inplace=True)

dataset.info(),dataset_non_Augmented.info(),dataset_Augmented.info()

"""## REMOVE"""

dataset_sequential_whole = dataset.copy() 
dataset_sequential_Augmented = dataset_Augmented.copy()
dataset_sequential_non_Augmented = dataset_non_Augmented.copy()
dataset

plt.figure(figsize = (33,33))
sns.heatmap(dataset.corr().round(1), annot = True)

dataset["Diagnosis"] = dataset["Diagnosis"].astype('category')
dataset["Diagnosis"] = dataset["Diagnosis"].cat.codes
dataset['Binary_diagnosis'] = (dataset['Binary_diagnosis'] == "Healthy").astype(int)

dataset_non_Augmented["Diagnosis"] = dataset_non_Augmented["Diagnosis"].astype('category')
dataset_non_Augmented["Diagnosis"] = dataset_non_Augmented["Diagnosis"].cat.codes
dataset_non_Augmented['Binary_diagnosis'] = (dataset_non_Augmented['Binary_diagnosis'] == "Healthy").astype(int)

dataset_Augmented["Diagnosis"] = dataset_Augmented["Diagnosis"].astype('category')
dataset_Augmented["Diagnosis"] = dataset_Augmented["Diagnosis"].cat.codes
dataset_Augmented['Binary_diagnosis'] = (dataset_Augmented['Binary_diagnosis'] == "Healthy").astype(int)

"""Cat codes work by doing a label encoding alphabetically, so URTI will be 5 because it is lowest """

correlation_heatmap = dataset[['Patient number', 'Sex' ,	'Age'	,'Diagnosis',	'Binary_diagnosis',	'zero_crossing',	'centroids',	'energy',	'new BMI']]
plt.figure(figsize = (33,33))
sns.heatmap(correlation_heatmap.corr().round(1), annot = True)

print("Number of Healthy Patients: ",(dataset['Binary_diagnosis'] == 1).sum())
print("Number of Unhealthy Patients: ",(dataset['Binary_diagnosis'] == 0).sum())
print("\n")
#print("Number of Patients with Asthma are: ",(dataset['Diagnosis'] == 0).sum())
print("Number of Patients with Bronchiectasis are: ",(dataset['Diagnosis'] == 0).sum())
print("Number of Patients with Bronchiolitis are: ",(dataset['Diagnosis'] == 1).sum())
print("Number of Patients with COPD are: ",(dataset['Diagnosis'] == 2).sum())
print("Number of Patients that are Healthy, are: ",(dataset['Diagnosis'] == 3).sum())
#print("Number of Patients with LRTI are: ",(dataset['Diagnosis'] == 5).sum())
print("Number of Patients with Pneumonia are: ",(dataset['Diagnosis'] == 4).sum())
print("Number of Patients with URTI are: ",(dataset['Diagnosis'] == 5).sum())

"""## Multi Classification of Whole Dataset"""

features = dataset.drop(columns = ['Diagnosis','Binary_diagnosis','Patient number','Recording index',"rand int i"])

targets = dataset[['Diagnosis']]

X_train, X_test, y_train, y_test=train_test_split(features, targets, test_size=0.2) # rand state sets a seed so that it will be the same

X_train_whole_dataset = X_train.values
X_test_whole_dataset = X_test.values

y_train_multi_whole_dataset = y_train.values.reshape(-1,)
y_test_multi_whole_dataset = y_test.values.reshape(-1,)

"""## Binary Classification of Whole Dataset"""

# targets = dataset[['Binary_diagnosis']]

# y = targets.values.reshape(-1,)

y_train_binary_whole_dataset = (y_train["Diagnosis"] == 4).astype(int).values.reshape(-1,)
y_test_binary_whole_dataset = (y_test["Diagnosis"] == 4).astype(int).values.reshape(-1,)

"""## Multi Classification of Augmented Dataset"""

features = dataset_Augmented.drop(columns = ['Diagnosis','Binary_diagnosis','Patient number','Recording index',"rand int i"])
targets = dataset_Augmented[['Diagnosis']]

X_train, X_test, y_train, y_test=train_test_split(features, targets, test_size=0.2 , random_state = 5000) # rand state sets a seed so that it will be the same

X_train_augmented_dataset = X_train.values
X_test_augmented_dataset = X_test.values

y_train_multi = y_train.values.reshape(-1,)
y_test_multi = y_test.values.reshape(-1,)

"""## Binary Classification of Augmented Dataset

"""

y_train_binary = (y_train["Diagnosis"] == 4).astype(int).values.reshape(-1,)
y_test_binary = (y_test["Diagnosis"] == 4).astype(int).values.reshape(-1,)

"""Training on the augmented dataset will be interesting, especially in relation to non-augmented

##Non-Augmented Dataset

We want to use the same data rows, so all we do is change the X_trains to be from the non augmented dataset, now the binary and multi classification values will stay the same
"""

X_train_non = dataset_non_Augmented.loc[X_train.index].drop(columns = ['Diagnosis','Binary_diagnosis','Patient number','Recording index',"rand int i"])
X_test_non = dataset_non_Augmented.loc[X_test.index].drop(columns = ['Diagnosis','Binary_diagnosis','Patient number','Recording index',"rand int i"])

X_train_non_augmented_dataset =  X_train_non.values
X_test_non_augmented_dataset = X_test_non.values

"""Training on the augmented dataset will be interesting, especially in relation to non-augmented

# Birch

Firstly, we will work with the augmented and non-augmented datasets that have been split.
"""

from sklearn.decomposition import PCA
from sklearn.cluster import Birch

"""### Binary Diagnosis - Whole Data"""

pca = PCA(2)

X_pca_train = pca.fit_transform(X_train_whole_dataset)
X_pca_test = pca.fit_transform(X_test_whole_dataset)
brc = Birch(n_clusters = 2) 
brc.fit(X_pca_train)

y_train_predict = brc.predict(X_pca_train)
y_test_predict = brc.predict(X_pca_test)

u_labels = np.unique(y_train_predict)
centroids = brc.subcluster_centers_

for i in u_labels:
    plt.scatter(X_pca_train[y_train_predict == i , 0] , X_pca_train[y_train_predict == i , 1] , label = i)

plt.legend()
plt.show()

"""As we can notice, it is classifying everything as 1, which for us was healthy. So I will do a calculation to switch up the labels to their proper label.\
Since we know that Unhealthy is vastly bigger
"""

y_train_predict[:] = [abs(x - 1) for x in y_train_predict]
y_test_predict[:] = [abs(x - 1) for x in y_test_predict]

plt.scatter(centroids[:,0] , centroids[:,1] , s = 40, color = 'k')
plt.legend()
plt.show()
# Each of these points then is it its own cluster denoted as subcluster in the birch algorithm since, it then gets clustered again based off of distance from each "leaf"

confusion_matrix_train = contingency_matrix(y_train_binary_whole_dataset,y_train_predict)
confusion_matrix_test = contingency_matrix(y_test_binary_whole_dataset,y_test_predict)

confusion = [confusion_matrix_train,confusion_matrix_test]
title = ["confusion_matrix_train","confusion_matrix_test"]
for x in confusion:
  print(x)

for i in range(2):
  fig, ax = plt.subplots(figsize=(10,10))
  sns.heatmap(confusion[i], annot=True)
  plt.xlabel('Predicted')
  plt.ylabel('Actual')
  plt.title(title[i])
  plt.show()

print("**Training Score:** {}\\".format(accuracy_score(y_train_binary_whole_dataset,y_train_predict)))
print("**Test Score:** {}\\".format(accuracy_score(y_test_binary_whole_dataset,y_test_predict)))

"""### Multi Diagnosis - Whole Data"""

brc = Birch(n_clusters=6)

brc.fit(X_pca_train)

y_train_predict = brc.predict(X_pca_train)
y_test_predict = brc.predict(X_pca_test)

u_labels = np.unique(y_train_predict)
centroids = brc.subcluster_centers_

for i in u_labels:
    plt.scatter(X_pca_train[y_train_predict == i , 0] , X_pca_train[y_train_predict == i , 1] , label = i)
 

plt.legend()
plt.show()

confusion_matrix_train = contingency_matrix(y_train_multi_whole_dataset,y_train_predict)
confusion_matrix_test = contingency_matrix(y_test_multi_whole_dataset,y_test_predict)

confusion_matrix_train = confusion_matrix_train[:, [2, 1, 0, 3, 4,5]]
confusion_matrix_test = confusion_matrix_test[:, [2, 1, 0, 3, 4,5]]

confusion = [confusion_matrix_train,confusion_matrix_test]
title = ["confusion_matrix_train","confusion_matrix_test"]

for i in range(2):
  fig, ax = plt.subplots(figsize=(10,10))
  sns.heatmap(confusion[i], annot=True)
  plt.xlabel('Predicted')
  plt.ylabel('Actual')
  plt.title(title[i])
  plt.show()

print("**Training Score:** {}\\".format(accuracy_score(y_train_multi_whole_dataset,y_train_predict)))
print("**Test Score:** {}\\".format(accuracy_score(y_test_multi_whole_dataset,y_test_predict)))

"""### Binary Diagnosis - Non Augmented"""

X_pca_train_non_augmented = pca.fit_transform(X_train_non_augmented_dataset)
X_pca_test_non_augmented = pca.fit_transform(X_test_non_augmented_dataset)

brc = Birch(n_clusters=2)

brc.fit(X_pca_train_non_augmented)

y_train_predict = brc.predict(X_pca_train_non_augmented)

y_test_predict = brc.predict(X_pca_test_non_augmented)

u_labels = np.unique(y_train_predict)
centroids = brc.subcluster_centers_

for i in u_labels:
    plt.scatter(X_pca_train_non_augmented[y_train_predict == i , 0] , X_pca_train_non_augmented[y_train_predict == i , 1] , label = i)
 

plt.legend()
plt.show()

"""Similar thing here where our data is clustered to be 1, but we know from fact that this must be Unhealthy and so I will correct the labels to be consistent with our own ground truth labels"""

y_train_predict[:] = [abs(x - 1) for x in y_train_predict]
y_test_predict[:] = [abs(x - 1) for x in y_test_predict]

confusion_matrix_train = contingency_matrix(y_train_binary,y_train_predict)
confusion_matrix_test = contingency_matrix(y_test_binary,y_test_predict)

confusion = [confusion_matrix_train,confusion_matrix_test]
title = ["confusion_matrix_train_non_augment","confusion_matrix_test_non_augment"]
for x in confusion:
  print(x)

"""We see that our contingency matrix is just a confusion matrix, without the labels so lets make it look nice, however before we do that we also notice that their predicted cluster put everything on 1, but in actuality that clustering label is unhealthy, and is defined as 0 in our dataset"""

for i in range(2):
  fig, ax = plt.subplots(figsize=(10,10))
  sns.heatmap(confusion[i], annot=True)
  plt.xlabel('Predicted')
  plt.ylabel('Actual')
  plt.title(title[i])
  plt.show()

print("**Training Score:** {}\\".format(accuracy_score(y_train_binary,y_train_predict)))
print("**Test Score:** {}\\".format(accuracy_score(y_test_binary,y_test_predict)))

"""### Multi Diagnosis - Non Augmented"""

brc = Birch(n_clusters=6)

brc.fit(X_pca_train_non_augmented)

y_train_predict = brc.predict(X_pca_train_non_augmented)
y_test_predict = brc.predict(X_pca_test_non_augmented)

u_labels = np.unique(y_train_predict)
centroids = brc.subcluster_centers_

for i in u_labels:
    plt.scatter(X_pca_train_non_augmented[y_train_predict == i , 0] , X_pca_train_non_augmented[y_train_predict == i , 1] , label = i)


plt.legend()
plt.show()

confusion_matrix_train = contingency_matrix(y_train_multi,y_train_predict)
confusion_matrix_test = contingency_matrix(y_test_multi,y_test_predict)

confusion_matrix_train = confusion_matrix_train[:, [0, 1, 5, 3, 4,2]]
confusion_matrix_test = confusion_matrix_test[:, [0, 1, 5, 3, 4,2]]
confusion = [confusion_matrix_train,confusion_matrix_test]
title = ["confusion_matrix_train","confusion_matrix_test"]

for i in range(2):
  fig, ax = plt.subplots(figsize=(10,10))
  sns.heatmap(confusion[i], annot=True)
  plt.xlabel('Predicted')
  plt.ylabel('Actual')
  plt.title(title[i])
  plt.show()

print("**Training Score:** {}\\".format(accuracy_score(y_train_multi,y_train_predict)))
print("**Test Score:** {}\\".format(accuracy_score(y_test_multi,y_test_predict)))

"""### Binary Diagnosis - Augmented"""

X_pca_train_augmented = pca.fit_transform(X_train_augmented_dataset)
X_pca_test_augmented = pca.fit_transform(X_test_augmented_dataset)

brc = Birch(n_clusters=2)

cluster = brc.fit(X_pca_train_augmented)

y_train_predict = brc.predict(X_pca_train_augmented)

y_test_predict = brc.predict(X_pca_test_augmented)

u_labels = np.unique(y_train_predict)
centroids = brc.subcluster_centers_

for i in u_labels:
    plt.scatter(X_pca_train_augmented[y_train_predict == i , 0] , X_pca_train_augmented[y_train_predict == i , 1] , label = i)

plt.legend()
plt.show()

y_train_predict[:] = [abs(x - 1) for x in y_train_predict]
y_test_predict[:] = [abs(x - 1) for x in y_test_predict]

confusion_matrix_train = contingency_matrix(y_train_binary,y_train_predict)
confusion_matrix_test = contingency_matrix(y_test_binary,y_test_predict)

confusion = [confusion_matrix_train,confusion_matrix_test]
title = ["confusion_matrix_train_non_augment","confusion_matrix_test_non_augment"]
for x in confusion:
  print(x)

for i in range(2):
  fig, ax = plt.subplots(figsize=(10,10))
  sns.heatmap(confusion[i], annot=True)
  plt.xlabel('Predicted')
  plt.ylabel('Actual')
  plt.title(title[i])
  plt.show()

print("**Training Score:** {}\\".format(accuracy_score(y_train_binary,y_train_predict)))
print("**Test Score:** {}\\".format(accuracy_score(y_test_binary,y_test_predict)))

"""### Multi Diagnosis - Augmented"""

brc = Birch(n_clusters=6)

brc.fit(X_pca_train_augmented)

y_train_predict = brc.predict(X_pca_train_augmented)
y_test_predict = brc.predict(X_pca_test_augmented)

u_labels = np.unique(y_train_predict)
centroids = brc.subcluster_centers_

for i in u_labels:
    plt.scatter(X_pca_train_augmented[y_train_predict == i , 0] , X_pca_train_augmented[y_train_predict == i , 1] , label = i)
 

plt.legend()
plt.show()

confusion_matrix_train = contingency_matrix(y_train_multi,y_train_predict)
confusion_matrix_test = contingency_matrix(y_test_multi,y_test_predict)

confusion_matrix_train

confusion_matrix_train = confusion_matrix_train[:, [0, 2, 1, 3, 4,5]]
confusion_matrix_test = confusion_matrix_test[:, [0, 2, 1, 3, 4,5]]

confusion = [confusion_matrix_train,confusion_matrix_test]
title = ["confusion_matrix_train","confusion_matrix_test"]

for i in range(2):
  fig, ax = plt.subplots(figsize=(10,10))
  sns.heatmap(confusion[i], annot=True)
  plt.xlabel('Predicted')
  plt.ylabel('Actual')
  plt.title(title[i])
  plt.show()

print("**Training Score:** {}\\".format(accuracy_score(y_train_multi,y_train_predict)))
print("**Test Score:** {}\\".format(accuracy_score(y_test_multi,y_test_predict)))